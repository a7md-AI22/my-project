# -*- coding: utf-8 -*-
"""Toxic Comment Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nkd3o0VFcamtcrxrRgfYFcg5xp-q07yY

Toxic Comment Classifier
"""

# رفع ملف kaggle.json
from google.colab import files
files.upload()

# إعداد وتحميل البيانات
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# تحميل الداتا
!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge

# فك الضغط
!unzip jigsaw-toxic-comment-classification-challenge.zip

import zipfile

# فك الضغط لملف train.csv.zip
with zipfile.ZipFile("train.csv.zip", 'r') as zip_ref:
    zip_ref.extractall()
import os

print("train.csv موجود؟", os.path.exists("train.csv"))

import pandas as pd

# قراءة الملف
df = pd.read_csv("train.csv")

# عرض أول 5 صفوف
df.head()



import pandas as pd

# قراءة ملف البيانات
df = pd.read_csv("train.csv")

# عرض أول 5 صفوف
print(df.head())

# عرض أسماء الأعمدة
print("\nأسماء الأعمدة:\n", df.columns.tolist())

# عدد الصفوف
print(f"\nعدد التعليقات: {len(df)}")

# عرض توزيع الفئات
print("\nتوزيع الفئات:")
print(df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum())

# عدد التعليقات غير المسيئة
df['non_toxic'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) == 0)
print(f"\nعدد التعليقات غير المسيئة: {df['non_toxic'].sum()}")

import re
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
!pip install emoji

import emoji
import re
from nltk.corpus import stopwords
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

def clean_text(text):
    # 1. تحويل الإيموجي لكلمات
    text = emoji.demojize(text)

    # 2. تحويل للحروف الصغيرة
    text = text.lower()

    # 3. إزالة الروابط
    text = re.sub(r"http\S+", "", text)

    # 4. إزالة الرموز الغريبة (مع الإبقاء على الإيموجي النصية مثل ":smile:")
    text = re.sub(r"[^a-z:_\s]", "", text)

    # 5. إزالة الأرقام
    text = re.sub(r"\d+", "", text)

    # 6. إزالة الكلمات الشائعة
    words = text.split()
    words = [word for word in words if word not in stop_words]

    # 7. إعادة النص
    return " ".join(words)

# إعادة إنشاء العمود cleaned_text
df['cleaned_text'] = df['comment_text'].apply(clean_text)

# عرض مثال قبل وبعد
print("قبل التنظيف:\n", df['comment_text'].iloc[0])
print("\nبعد التنظيف:\n", df['cleaned_text'].iloc[0])

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# استخدام العمود cleaned_text كـ input، وlabel كـ output
# إنشاء عمود label: 1 لو التعليق فيه أي نوع إساءة، 0 لو نظيف
df['label'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0).astype(int)
X = df['cleaned_text']
y = df['label']

# تقسيم البيانات إلى تدريب واختبار (Train/Test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# إنشاء محول TF-IDF
tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))  # unigram + bigram

# تدريب وتحويل بيانات التدريب
X_train_tfidf = tfidf.fit_transform(X_train)

# تحويل بيانات الاختبار
X_test_tfidf = tfidf.transform(X_test)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# إنشاء وتدريب النموذج
model = LogisticRegression(max_iter=1000)
model.fit(X_train_tfidf, y_train)

# التوقع
y_pred = model.predict(X_test_tfidf)

# تقرير الأداء
print(classification_report(y_test, y_pred))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Toxic', 'Toxic'], yticklabels=['Not Toxic', 'Toxic'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# تدريب النموذج مع توازن الفئات
model = LogisticRegression(max_iter=1000, class_weight='balanced')
model.fit(X_train_tfidf, y_train)

# التوقع
y_pred = model.predict(X_test_tfidf)

# التقييم
print(classification_report(y_test, y_pred))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Toxic', 'Toxic'],
            yticklabels=['Not Toxic', 'Toxic'])

plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

pip install transformers datasets torch scikit-learn

import pandas as pd

# تحميل البيانات
df = pd.read_csv("train.csv")
df = df[['comment_text', 'toxic']].rename(columns={"toxic": "label"})
df = df.dropna()
df['label'] = df['label'].astype(int)

df.head()

from datasets import Dataset

dataset = Dataset.from_pandas(df)

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def tokenize_function(example):
    return tokenizer(example['comment_text'], truncation=True, padding='max_length', max_length=128)

dataset = dataset.map(tokenize_function, batched=True)

dataset = dataset.train_test_split(test_size=0.2)
train_dataset = dataset['train']
test_dataset = dataset['test']

from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

!pip install -U transformers

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    # لازم يكون مع الإصدار الحديث
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

trainer.train()
trainer.evaluate()

from sklearn.metrics import classification_report

# توقع على بيانات التست
predictions = trainer.predict(test_dataset)
y_pred = predictions.predictions.argmax(-1)
y_true = predictions.label_ids

print(classification_report(y_true, y_pred))
TrainingArguments(
    output_dir="./results",
    run_name="bert-toxic-classification-run1",  # اسم التجربة

)